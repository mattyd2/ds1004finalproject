{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reformat(data):\n",
    "    '''\n",
    "    wrangle stop Times data to get start and stop times between each stop\n",
    "    '''\n",
    "    \n",
    "    reformattedData =  data.copy()\n",
    "    tempTime= data['departure_time'].ix[1:].values\n",
    "    times = pd.Series(tempTime)\n",
    "    times.append(pd.Series(['2']))\n",
    "\n",
    "    tempSequence = data['stop_sequence'].ix[1:].values\n",
    "    sequence= pd.Series(tempSequence)\n",
    "    sequence.append(pd.Series(['2']))\n",
    "\n",
    "    start_id = data['stop_id'].ix[1:].values\n",
    "    ids = pd.Series(start_id)\n",
    "    ids.append(pd.Series(['2']))\n",
    "\n",
    "    reformattedData['stop_sequence2'] =sequence\n",
    "    reformattedData['departure_time'] = times\n",
    "    reformattedData['stop_id'] = ids\n",
    "    reformattedData['start_id'] = data['stop_id']\n",
    "    \n",
    "    goodIndexes = (reformattedData['stop_sequence'] +1 == reformattedData['stop_sequence2'])\n",
    "    reformattedData = reformattedData[goodIndexes]\n",
    "    return reformattedData\n",
    "     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#helper functions for cleaning up time when day changes\n",
    "def hh_mm_ss2seconds(hh_mm_ss):\n",
    "    return reduce(lambda acc, x: acc*60 + x, map(int, hh_mm_ss.split(':')))\n",
    "\n",
    "def cleanTimes(t):\n",
    "    '''\n",
    "    input: duration\n",
    "    output: if duration < 0 , adds 24 hrs \n",
    "    '''\n",
    "    if t<0:\n",
    "        return t+ 24*60*60\n",
    "    else :\n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "File ./data/rawData/bus/google_transit_brooklyn/stop_times.txt does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-5dbbb448c24e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#bus data for stops and stop times\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mbrookStopTimes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data/rawData/bus/google_transit_brooklyn/stop_times.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mqueensStopTimes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data/rawData/bus/google_transit_queens/stop_times.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mbronxStopTimes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data/rawData/bus/google_transit_bronx/stop_times.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, dialect, compression, doublequote, escapechar, quotechar, quoting, skipinitialspace, lineterminator, header, index_col, names, prefix, skiprows, skipfooter, skip_footer, na_values, true_values, false_values, delimiter, converters, dtype, usecols, engine, delim_whitespace, as_recarray, na_filter, compact_ints, use_unsigned, low_memory, buffer_lines, warn_bad_lines, error_bad_lines, keep_default_na, thousands, comment, decimal, parse_dates, keep_date_col, dayfirst, date_parser, memory_map, float_precision, nrows, iterator, chunksize, verbose, encoding, squeeze, mangle_dupe_cols, tupleize_cols, infer_datetime_format, skip_blank_lines)\u001b[0m\n\u001b[1;32m    496\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_options_with_defaults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    729\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 731\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    732\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1101\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1103\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas/parser.c:3246)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas/parser.c:6111)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: File ./data/rawData/bus/google_transit_brooklyn/stop_times.txt does not exist"
     ]
    }
   ],
   "source": [
    "# http://web.mta.info/developers/developer-data-terms.html#data\n",
    "# under GTFS schedule data, New York City Transit Bus and New York City Transit Train - last updated march 24 2016\n",
    "\n",
    "#bus data for stops and stop times\n",
    "brookStopTimes = pd.read_csv(\"./data/rawData/bus/google_transit_brooklyn/stop_times.txt\", nrows=1000)\n",
    "queensStopTimes = pd.read_csv(\"./data/rawData/bus/google_transit_queens/stop_times.txt\", nrows=1000)\n",
    "bronxStopTimes = pd.read_csv(\"./data/rawData/bus/google_transit_bronx/stop_times.txt\", nrows=1000)\n",
    "manhattanStopTimes = pd.read_csv(\"./data/rawData/bus/google_transit_manhattan/stop_times.txt\", nrows=1000)\n",
    "statStopTimes = pd.read_csv(\"./data/rawData/bus/google_transit_staten_island/stop_times.txt\", nrows=1000)\n",
    "\n",
    "\n",
    "brookStops = pd.read_csv(\"./data/rawData/bus/google_transit_brooklyn/stops.txt\", nrows=1000)\n",
    "queensStops = pd.read_csv(\"./data/rawData/bus/google_transit_queens/stops.txt\", nrows=1000)\n",
    "bronxStops = pd.read_csv(\"./data/rawData/bus/google_transit_bronx/stops.txt\", nrows=1000)\n",
    "manhattanStops = pd.read_csv(\"./data/rawData/bus/google_transit_manhattan/stops.txt\", nrows=1000)\n",
    "statStops = pd.read_csv(\"./data/rawData/bus/google_transit_staten_island/stops.txt\", nrows=1000)\n",
    "\n",
    "\n",
    "\n",
    "#all train data\n",
    "stopTimes = pd.read_csv(\"./data/rawData/train/google_transit/stop_times.txt\", nrows=1000)\n",
    "stops = pd.read_csv(\"./data/rawData/train/google_transit/stops.txt\", nrows=1000)\n",
    "routes = pd.read_csv(\"./data/rawData/train/google_transit/routes.txt\", nrows=1000)\n",
    "trips = pd.read_csv(\"./data/rawData/train/google_transit/trips.txt\", nrows=1000)\n",
    "\n",
    "\n",
    "#clean busTrips and trainTrips\n",
    "bronx= reformat(bronxStopTimes)\n",
    "stat= reformat(statStopTimes)\n",
    "man =reformat(manhattanStopTimes)\n",
    "queens = reformat(queensStopTimes)\n",
    "brook = reformat(brookStopTimes)\n",
    "\n",
    "allRoutesTrain = reformat(stopTimes)\n",
    "allRoutesTrain['type']=\"train\"\n",
    "\n",
    "\n",
    "allRoutesBus = pd.concat([bronx,stat,man,queens,brook])\n",
    "allRoutesBus['type']=\"bus\"\n",
    "\n",
    "\n",
    "busStops= pd.concat([brookStops,queensStops,bronxStops,manhattanStops,statStops])\n",
    "busStops['type']=\"busStop\"\n",
    "stops['type']=\"trainStop\"\n",
    "\n",
    "#combine all bus and train stops/trips\n",
    "allStops = pd.concat([stops,busStops])\n",
    "allRoutes = pd.concat([allRoutesTrain,allRoutesBus])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'allRoutes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3aacea734835>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# merge and rename dataframes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mallRoutes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallStops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'stop_id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'stop_lat'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'dropoff_latitude'\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m'stop_lon'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'dropoff_longitude'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mresult2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallStops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_on\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'start_id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mright_on\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'stop_id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mresult2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'stop_lat'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'pickup_latitude'\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m'stop_lon'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'pickup_longitude'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'allRoutes' is not defined"
     ]
    }
   ],
   "source": [
    "# merge and rename dataframes\n",
    "result = pd.merge(allRoutes, allStops, on='stop_id')\n",
    "result.rename(columns = {'stop_lat':'dropoff_latitude' , 'stop_lon':'dropoff_longitude'}, inplace = True)\n",
    "result2 = pd.merge(result, allStops, left_on='start_id',right_on='stop_id')\n",
    "result2.rename(columns = {'stop_lat':'pickup_latitude' , 'stop_lon':'pickup_longitude'}, inplace = True)\n",
    "result2= result2[['trip_id','stop_id_x','start_id','type','arrival_time','departure_time']]\n",
    "result2.rename(columns = {'start_id':'mta_start_id','stop_id_x':'mta_stop_id'}, inplace = True)\n",
    "allStops.rename(columns = {'stop_id':'id','stop_lat':'lat','stop_lon':'long'}, inplace = True)\n",
    "allStops =allStops[['id','lat','long','type']]\n",
    "allStops = allStops.reset_index()\n",
    "\n",
    "# output of original data merged\n",
    "allStops.to_csv('./data/mergedData/busTrainStops.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#compute duration of trip\n",
    "times = pd.DataFrame()\n",
    "times['ar'] = result2['arrival_time'].apply(lambda x:hh_mm_ss2seconds(x) )\n",
    "times['de'] = result2['departure_time'].apply(lambda x:hh_mm_ss2seconds(x) )\n",
    "times['dif']= times['de']-times['ar']\n",
    "\n",
    "#handle case where time stamp switched between days\n",
    "result2['duration'] = times['dif'].apply(lambda x: cleanTimes(x)/60.)\n",
    "result2 = result2[['mta_stop_id','mta_start_id','type','duration']]\n",
    "result2.to_csv('./data/mergedData/busTrainTrips.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stopReducer(data,radius):\n",
    "    '''\n",
    "    performs one pass through the stop locations, combining stops that are within the radius \n",
    "    (manhattan distance). If a stop is assigned it is not eligible to be assigned to a new group.\n",
    "    Basic function is to combine points that are close together\n",
    "    '''\n",
    "    reducerMap = dict()\n",
    "    \n",
    "    data['assigned'] = \"a\"\n",
    "    groupNumber =0\n",
    "    #data[mask]['assigned'].apply(lambda x:  False)\n",
    "    rowNumber = 0\n",
    "    while (data['assigned']==\"a\").sum()>0:  #continue until all of data has been assigned to a group\n",
    "        row = data.iloc[rowNumber]\n",
    "        if row['assigned']==\"a\":  # has not been assigned yet\n",
    "            \n",
    "            # create filter for \"close points\" so we don't have to look at all pairs\n",
    "            data[\"x\"] = data['lat'].apply(lambda x: np.absolute(x - row.lat))\n",
    "            data[\"y\"] = data['long'].apply(lambda x: np.absolute(x - row['long']))\n",
    "            data['L1'] = data[\"x\"] + data[\"y\"]\n",
    "            mask0= data['roughLatLon']==row.roughLatLon\n",
    "            mask1 = data['L1']<radius\n",
    "            \n",
    "            #only want to combine with other points that have not been assigned\n",
    "            mask2 = data['assigned']==\"a\"\n",
    "            \n",
    "            mask3 = np.logical_and(mask1, mask2)\n",
    "            mask4 = np.logical_and(mask3,mask0)\n",
    "            \n",
    "            data.ix[mask4,'assigned']=groupNumber\n",
    "            \n",
    "            groupNumber += 1\n",
    "        rowNumber +=1\n",
    "\n",
    "    \n",
    "    return   data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allStops['assigned']=\"a\" # mark unassigned rows\n",
    "allStops['roughLat']=allStops['lat'].apply(lambda x: float(int(x*100))/100)\n",
    "allStops['roughLon']=allStops['long'].apply(lambda x: float(int(x*100))/100)\n",
    "allStops['roughLatLon']=zip(allStops['roughLat'] , allStops['roughLon'])\n",
    "allStops['assigned']=\"a\"\n",
    "\n",
    "a = stopReducer(allStops,.002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14201"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reducedStops = a[['lat','long','type','assigned','id']]\n",
    "reducedStops['id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2902: DtypeWarning: Columns (1,2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "reducedStops.to_csv('./data/mergedData/reducedStops.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
